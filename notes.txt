DeepStack

-F, C, P, A, actions provide an excellent tradeoff between computational requirements via the size of the solved lookahead tree and approximation quality.

- Continual re-solving does not require keeping track of
the opponent’s range. The re-solving step essentially reconstructs a suitable range using the
bounded counterfactual values. In particular, the CFR-D gadget does this by giving the opponent the option, after being dealt a uniform random hand, of terminating the game (T) instead
of following through with the game (F), allowing them to simply earn that hand’s bound on
its counterfactual value. Only hands which are valuable to bring into the subgame will then
be observed by the re-solving player.
	-However, this process of the opponent learning which hands to follow through with can make re-solving require many iterations. An estimate of the opponent’s range can be used to effectively warm-start the choice of opponent ranges, and help speed up the re-solving.
	-Read -> Opponent Ranges in Re-Solving.
		Supplementary material

-The training situations were generated by first sampling a pot size from a fixed distribution
which was designed to approximate observed pot sizes from older HUNL programs.1

-Random ranges using function

-NN training , the situations were approximately
solved using 1,000 iterations of CFR+ with only betting actions fold, call, a pot-sized bet, and
all-in

	-Find a way to train NN faster, use 	Pytorch, maybe use card abstraction(then must use card abstr. for real time,abstract by clustering hands using k means) and simplify CFR. First train turn NN, then use turn NN to train flop NN. don't generate ranges wich are not likely to occur. Build aditional NN to aproximate river CFR values. Maybe use auxiliary networks for each NN to aproximate their values for the first iterations(Doesn't improve training duh, only real time computation). Use 5 hidden layers max "as the validation error does not improve much beyond five".
!!!Use resolve to find wich ranges are more common among each public state and then keep training the pre-semi-trained NN!!!!

-Range, given past range, action and strategy compute probability of hand, AKA range using ¿bayes rule?.

-Build the input data for the NN by first generating random ranges, boards and pots for the previous street, then compute resolve to get appropiate ranges for the input of the NN. If player's P(folding) is too high after some iters stop cfr iterations of initial street don't use that branch for NN input, and focus on more likely to reach NN's street branches.

-can use pypoker eval for faster evaluation.

-Regret should be higher when pot is higher and you lose. otherwise bot will always bet high to increase pot and utility but will not
be penalized for losing more. Utility can be negeative. 
Note: the higher the pot the higher the regret for losing since losing utility~0 and regret -> 0 - averageUtility(higher with pot).

-TODO:On ReSolve, make bot use check instead of fold if to_call = 0 -> P(check) += P(fold), P(fold) = 0.

-Contemplate using softmax for normalization(softmax is used when there are negative arguments)



NEED TO SPECIFY MIN_RAISE -> https://poker.stackexchange.com/questions/2729/what-is-the-min-raise-and-min-reraise-in-holdem-no-limit
	raise is always Pot size thus higher than min_raise

-map actions to continous distributin between fold and all in. betting distribution p=0 at 0 amount, (p at pot)/(p at all_in) as in strategy.